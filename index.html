<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Aigerim Keutayeva</title>
    <meta name="google-site-verification" content="Oow2DyZhZrQha-6adTH1UADIutE6o0wg1lhUJ4qbpu8" />

    <meta name="author" content="Aigerim Keutayeva">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <script>
      function showBibtex(id) {
        var element = document.getElementById(id);
        if (element.style.display === "none" || !element.style.display) {
          element.style.display = "block";
        } else {
          element.style.display = "none";
        }
      }

      function copyToClipboard(id) {
        var element = document.getElementById(id);
        var text = element.innerText || element.textContent;
        navigator.clipboard.writeText(text).then(() => {
          alert("Copied to clipboard!");
        }).catch(err => {
          console.error("Failed to copy: ", err);
        });
      }
    </script>
    
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><circle cx=%2250%22 cy=%2250%22 r=%2248%22 fill=%22none%22 stroke=%22black%22/><text x=%2250%22 y=%2250%22 font-size=%2260%22 text-anchor=%22middle%22 dominant-baseline=%22middle%22>AK</text></svg>">

    
  </head>

  <body>
    <header>
      <div class="header-content">
        <h1 class="header-title">Aigerim Keutayeva</h1>
        <nav class="header-nav">
          <a href="#bio">Home</a>
          <a href="#publications">Publications</a>
          <a href="#projects">Projects</a>
          <a href="#contact">Contact</a>
        </nav>
      </div>
    </header>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <section id="bio">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:top">
                  <p class="name" style="text-align: center; font-weight: bold;">
                    About me
                  </p>
                  <p>
                    I am a Full-time Research Assistant at <a href="https://brainu.notion.site/">Brain-Machine Interfaces Lab</a> in Astana, Kazakhstan.
                  </p>
                  <p>
                    I received my <a href="https://seds.nu.edu.kz/bachelor_in_re">B.S. degree in Robotics and Mechatronics</a> and <a href="https://seds.nu.edu.kz/msc_in_robotics">M.S. degree in Robotics</a> from Nazarbayev University, Astana, Kazakhstan, in 2021 and 2023, respectively.
                    Since 2019, I have been a Research Assistant with the <a href="https://seds.nu.edu.kz/">School of Engineering and Digital Sciences</a>, and a member of <a href="https://yrascience.kz/">Young Researchers Alliance</a>. 
                    My Master's journey was advised by <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev">Berdakh Abibullaev</a> and funded by the <a href="http://seds.nu.edu.kz">NU Research Grant Program</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:akeutayeva@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/Aigerim_CV.pdf">CV</a> &nbsp;/&nbsp;
                    <!-- <a href="data/Aigerim-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=3-MEnzYAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/aikesha/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:35%;max-width:40%; margin-top:0px; ">
                  <a href="images/a22.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/a25.jpg" class="hoverZoomLink"></a>
                  <p style="font-size: 18px; text-align: center;">
                    Full-time Research Assistant
                    <a href="https://seds.nu.edu.kz/" style="font-size: 16px;">Nazarbayev University</a>
                  </p>
                </td>
              </tr>
            </tbody></table>

            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:top">
                    <h2 style="margin-bottom:10px;">üìù Research interests</h2>
                    <ul style="list-style-type:disc; padding-left:40px;margin:0;">
                      <li style="margin-bottom:5px;">Machine Learning</li>
                      <li style="margin-bottom:5px;">Brain&ndash;Computer Interfaces</li>
                      <li style="margin-bottom:5px;">Signal Processing</li>
                      <li style="margin-bottom:5px;">Human-Robot Interaction</li>
                      <li style="margin-bottom:5px;">Robotics</li>
                      <li style="margin-bottom:5px;">Digital Twins</li>
                    </ul>
                  </td>
                  
                  <td style="padding:20px;width:55%;vertical-align:top">
                    <h2 style="margin-bottom:10px;">üéì Education</h2>
                    <ul style="list-style-type:disc; list-style-position:outside; padding-left:40px; margin:0;">
                      <li style="margin-bottom:15px;">
                        <strong>Master of Science in Robotics, 2023</strong><br>
                        <em>Graduated with Honors (top 10%)</em><br>
                        Nazarbayev University
                      </li>
                      <li>
                        <strong>Bachelor of Science in Robotics and Mechatronics, 2021</strong><br>
                        Nazarbayev University
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </section>

          <section id="publications">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>üìö Publications</h2>
                <h3 style="font-weight: bold;">Note: The code for the following publications is currently being finalized and will be uploaded to GitHub soon.</h3>

              </td>
              </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h3>Journal Articles (peer-reviewed)</h3>
                <p>
                  Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
              </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- Publication 0 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src='images/vep_review.PNG' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10976640">
                    <span class="papertitle">Neurotechnology in Gaming: A Systematic Review of Visual Evoked Potential-Based Brain-Computer Interfaces</span>
                  </a>
                  <br>
                  <strong>Aigerim Keutayeva</strong>,
                  China Jesse Nwachukwu, 
                  Muslim Alaran, 
                  Zhenis Otarbay,
                  <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev"> Berdakh Abibullaev</a>,
                  <br>
                  <em>IEEE Access</em>, April 25, 2025
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract10')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex10')"><i class="fa fa-book"></i> bib</a>
                  </div>
                  <div id="abstract10" class="text-block" style="display: none;">
                     Brain-computer interfaces (BCIs) have received considerable attention in gaming, enabling innovative interactions with digital environments. Visual Evoked Potentials (VEPs)‚Äîrobust, noninvasive neural responses to visual stimuli‚Äîoffer high information transfer rates, making them particularly promising. This systematic review, guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework, examines VEP-based BCIs in gaming. We searched the Web of Science and Google Scholar, identifying 16 347 studies from the past decade, with 46 selected for in-depth analysis after rigorous screening. The review explores VEP response modeling, electroencephalography (EEG) signal acquisition and processing, stimulation paradigms, and their gaming applications. These systems enhance accessibility for players with physical or cognitive impairments, support adaptive difficulty scaling, personalize gameplay, aid neurorehabilitation, and enable multiplayer interactions. However, challenges remain, including technical limitations, complex data interpretation, user adaptability, and ergonomic issues. Advances in signal processing, personalized calibration, and hybrid multimodal approaches could improve usability. Future research should focus on integrating VEP-based BCIs with emerging technologies, optimizing user comfort, and developing adaptive interaction models to enhance immersion and accessibility. By addressing these challenges and utilizing neuroscience and computational advancements, VEP-based BCIs promise to transform gaming into a more inclusive and immersive experience for diverse users.
                  </div>
                  <div id="bibtex10" class="text-block" style="display: none;">
                    @ARTICLE{Keutayeva_Neurotech_2025,
                    author={Keutayeva, Aigerim and Jesse Nwachukwu, China and Alaran, Muslim and Otarbay, Zhenis and Abibullaev, Berdakh},
                    journal={IEEE Access}, 
                    title={Neurotechnology in Gaming: A Systematic Review of Visual Evoked Potential-Based Brain-Computer Interfaces}, 
                    year={2025},
                    volume={13},
                    number={},
                    pages={74944-74966},
                    doi={10.1109/ACCESS.2025.3564328}}
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex10')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    The review explores VEP response modeling, electroencephalography (EEG) signal acquisition and processing, stimulation paradigms, and their gaming applications. 
                  </p>
                </td>
              </tr>
              
              
              
              <!-- Publication 1 -->              
              <tr onmouseout="zipnerf_stop0()" onmouseover="zipnerf_start0()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src='images/compact.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.nature.com/articles/s41598-024-73755-4">
                    <span class="papertitle">Compact convolutional transformer for subject-independent motor imagery EEG-based BCIs</span>
                  </a>
                  <br>
                  <strong>Aigerim Keutayeva</strong>,
                  <a href="https://ieeexplore.ieee.org/author/272643398525040"> Nail Fakhrutdinov</a>,
                  <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev"> Berdakh Abibullaev</a>
                  <br>
                  <em>Scientific Reports</em>, October 28, 2024
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract11')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex11')"><i class="fa fa-book"></i> bib</a>
                  </div>
                  <div id="abstract11" class="text-block" style="display: none;">
                    Motor imagery electroencephalography (EEG) analysis is crucial for the development of effective brain-computer interfaces (BCIs), yet it presents considerable challenges due to the complexity of the data and inter-subject variability. This paper introduces EEGCCT, an application of compact convolutional transformers designed specifically to improve the analysis of motor imagery tasks in EEG. Unlike traditional approaches, EEGCCT model significantly enhances generalization from limited data, effectively addressing a common limitation in EEG datasets. We validate and test our models using the open-source BCI Competition IV datasets 2a and 2b, employing a Leave-One-Subject-Out (LOSO) strategy to ensure subject-independent performance. Our findings demonstrate that EEGCCT not only outperforms conventional models like EEGNet in standard evaluations but also achieves better performance compared to other advanced models such as Conformer, Hybrid s-CViT, and Hybrid t-CViT, while utilizing fewer parameters and achieving an accuracy of 70.12%. Additionally, the paper presents a comprehensive ablation study that includes targeted data augmentation, hyperparameter optimization, and architectural improvements.
                  </div>
                  <div id="bibtex11" class="text-block" style="display: none;">
                    @article{keutayeva2024compact,
                    title={Compact convolutional transformer for subject-independent motor imagery EEG-based BCIs},
                    author={Keutayeva, Aigerim and Fakhrutdinov, Nail and Abibullaev, Berdakh},
                    journal={Scientific Reports},
                    volume={14},
                    number={1},
                    pages={25775},
                    year={2024},
                    publisher={Nature Publishing Group UK London}
                    }
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex11')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    This paper introduces EEGCCT, an application of compact convolutional transformers designed specifically to improve the analysis of motor imagery tasks in EEG. 
                  </p>
                </td>
              </tr>
              
              
              <!-- Publication 2 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src='images/data_constraints_image.PNG' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10509679">
                    <span class="papertitle">Data Constraints and Performance Optimization for Transformer-based Models in EEG-based Brain-Computer Interfaces: A Survey</span>
                  </a>
                  <br>
                  <strong>Aigerim Keutayeva</strong>,
                  <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev"> Berdakh Abibullaev</a>,
                  <br>
                  <em>IEEE Access</em>, April 2024
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract0')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex0')"><i class="fa fa-book"></i> bib</a>
                  </div>
                  <div id="abstract0" class="text-block" style="display: none;">
                     This work reviews the critical challenge of data scarcity in developing Transformer-based models for Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs), specifically focusing on Motor Imagery (MI) decoding. While EEG-BCIs hold immense promise for applications in communication, rehabilitation, and human-computer interaction, limited data availability hinders the use of advanced deep-learning models such as Transformers. In particular, this paper comprehensively analyzes three key strategies to address data scarcity: data augmentation, transfer learning, and the inherent attention mechanisms of Transformers. Data augmentation techniques artificially expand datasets, enhancing model generalizability by exposing them to a wider range of signal patterns. Transfer learning utilizes pre-trained models from related domains, leveraging their learned knowledge to overcome the limitations of small EEG datasets. By thoroughly reviewing current research and methodologies, this work underscores the importance of these strategies in overcoming data scarcity. It critically examines the limitations imposed by limited datasets and showcases potential solutions being developed to address these challenges. This comprehensive survey, focusing on the intersection of data scarcity and technological advancements, aims to provide a critical analysis of the current state-of-the-art in EEG-BCI development. By identifying research gaps and suggesting future directions, the paper encourages further exploration and innovation in this field. Ultimately, this work aims to contribute to the advancement of more accessible, efficient, and accurate EEG-BCI systems by addressing the fundamental challenge of data scarcity.
                  </div>
                  <div id="bibtex0" class="text-block" style="display: none;">
                    @ARTICLE{Keutayeva_Review_2024,
                    author={Keutayeva, Aigerim and Abibullaev, Berdakh},
                    journal={IEEE Access}, 
                    title={Data Constraints and Performance Optimization for Transformer-Based Models in EEG-Based Brain-Computer Interfaces: A Survey}, 
                    year={2024},
                    volume={12},
                    number={},
                    pages={62628-62647},
                    doi={10.1109/ACCESS.2024.3394696}}
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex0')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    This work reviews the critical challenge of data scarcity in developing Transformer-based models for EEG-based BCIs, specifically focusing on Motor Imagery decoding. 
                  </p>
                </td>
              </tr>

              <!-- Publication 3 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src='images/back_lit_paper.PNG' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10305163">
                    <span class="papertitle">Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications</span>
                  </a>
                  <br>
                  <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev"> Berdakh Abibullaev</a>,
                  <strong>Aigerim Keutayeva</strong>,
                  <a href="https://research.nu.edu.kz/en/persons/amin-zollanvari"> Amin Zollanvari</a>,
                  <br>
                  <em>IEEE Access</em>, November 2023
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract1')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex1')"><i class="fa fa-book"></i> bib</a>
                    <!-- <a class="badge badge-slides" href="slides_link" target="_blank"><i class="fa fa-file-powerpoint-o"></i> slides</a>
                    <a class="badge badge-video" href="video_link" target="_blank"><i class="fa fa-video-camera"></i> video</a>
                    <a class="badge badge-code" href="code_link" target="_blank"><i class="fa fa-code"></i> code</a>
                    <a class="badge badge-demo" href="demo_link" target="_blank"><i class="fa fa-cogs"></i> demo</a>-->
                  </div>
                  <div id="abstract1" class="text-block" style="display: none;">
                    Brain-computer interfaces (BCIs) have undergone significant advancements in recent years. The integration of deep learning techniques, specifically transformers, has shown promising development in research and application domains. Transformers, which were originally designed for natural language processing, have now made notable inroads into BCIs, offering a unique self-attention mechanism that adeptly handles the temporal dynamics of brain signals. This comprehensive survey delves into the application of transformers in BCIs, providing readers with a lucid understanding of their foundational principles, inherent advantages, potential challenges, and diverse applications. In addition to discussing the benefits of transformers, we also address their limitations, such as computational overhead, interpretability concerns, and the data-intensive nature of these models, providing a well-rounded analysis. Furthermore, the paper sheds light on the myriad of BCI applications that have benefited from the incorporation of transformers. These applications span from motor imagery decoding, emotion recognition, and sleep stage analysis to novel ventures such as speech reconstruction. This review serves as a holistic guide for researchers and practitioners, offering a panoramic view of the transformative potential of transformers in the BCI landscape. With the inclusion of examples and references, readers will gain a deeper understanding of the topic and its significance in the field.
                  </div>
                  <div id="bibtex1" class="text-block" style="display: none;">
                    @ARTICLE{Abibullaev_Review_2023,
                    author={Abibullaev, Berdakh and Keutayeva, Aigerim and Zollanvari, Amin},
                    journal={IEEE Access}, 
                    title={Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications}, 
                    year={2023},
                    volume={11},
                    number={},
                    pages={127271-127301},
                    doi={10.1109/ACCESS.2023.3329678}}
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex1')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div> 
                  <p>
                    This comprehensive survey delves into the application of transformers in BCIs, providing readers with a lucid understanding of their foundational principles, inherent advantages, potential challenges, and diverse applications.
                  </p>
                </td>
              </tr>

              <!-- Publication 4 -->
              <tr bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src='images/CNN_ViT.PNG' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10266354">
                    <span class="papertitle">Exploring the Potential of Attention Mechanism-Based Deep Learning for Robust Subject-Independent Motor-Imagery Based BCIs</span>
                  </a>
                  <br>
                  <strong>Aigerim Keutayeva</strong>,
                  <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev"> Berdakh Abibullaev</a>
                  <br>
                  <em>IEEE Access</em>, September 2023
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract2')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex2')"><i class="fa fa-book"></i> bib</a>
                    <a class="badge badge-code" href="https://github.com/aikesha/Spatio-Temporal-CNN-ViT" target="_blank"><i class="fa fa-code"></i> code</a>
                  </div>
                  <div id="abstract2" class="text-block" style="display: none;">
                    This study explores the use of attention mechanism-based deep learning models to construct subject-independent motor-imagery based brain-computer interfaces (MI-BCIs), which present unique and intricate challenges from a machine learning perspective. By comparing four attention mechanism-based models and employing nested LOSO methods for robust model selection, the study enhances the reliability of performance estimates and offers unique insights into the application of attention mechanisms in building subject-independent BCIs. The results indicate the potential of the Spatio-Temporal CNN + ViT model for practical BCI applications, as it outperforms other models on several datasets. Additionally, the study presents a realistic approach to building subject-independent BCIs by combining attention mechanisms and deep learning models to identify informative features common across subjects while filtering out noise and irrelevant data. While there are limitations and areas for future work to enhance the potential of these models, transformer-based models could become even more valuable in the BCI research field, leading to more robust and accurate subject-independent BCIs for various applications. The need for subject-independent MI-BCIs is amplified due to their potential in assisting individuals with severe neurological conditions, such as ALS and locked-in syndrome, which severely limit mobility and communication.
                  </div>
                  <div id="bibtex2" class="text-block" style="display: none;">
                    @ARTICLE{Keutayeva_stCNN_ViT_2023,
                    author={Keutayeva, Aigerim and Abibullaev, Berdakh},
                    journal={IEEE Access}, 
                    title={Exploring the Potential of Attention Mechanism-Based Deep Learning for Robust Subject-Independent Motor-Imagery Based BCIs}, 
                    year={2023},
                    volume={11},
                    number={},
                    pages={107562-107580},
                    doi={10.1109/ACCESS.2023.3320561}}
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex2')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    This study explores the use of attention mechanism-based deep learning models to construct subject-independent motor-imagery based brain-computer interfaces (MI-BCIs), which present unique and intricate challenges from a machine learning perspective.
                  </p>
                </td>
              </tr>
              
              <!-- Publication 5 -->
              <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()" >
                <td style="padding:20px;width:25%;vertical-align:top">
                  <div class="one">
                    <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
                    <source src="images/digital-twin.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <img src='images/Digital_twin.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function zipnerf_start() {
                      document.getElementById('zipnerf_image').style.opacity = "1";
                    }
                    function zipnerf_stop() {
                      document.getElementById('zipnerf_image').style.opacity = "0";
                    }
                    zipnerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10179224">
                    <span class="papertitle">Integrating Machine Learning Model and Digital Twin System for Additive Manufacturing</span>
                  </a>
                  <br>
                  <a href="https://www.researchgate.net/profile/Nursultan_Jyeniskhan/research">Nursultan Jyeniskhan</a>,
                  <strong>Aigerim Keutayeva</strong>,
                  <a href="https://www.linkedin.com/in/gani-kazbek-4752091a2/?originalSubdomain=kz">Gani Kazbek</a>,
                  <a href="https://scholar.google.com.my/citations?user=W0H1Sy0AAAAJ&hl=en">Md.Hazrat Ali</a>,
                  <a href="https://scholar.google.co.uk/citations?user=mR9VYwgAAAAJ&hl=en">Essam Shehab</a>
                  <br>
                  <em>IEEE Access</em>, July 2023
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract3')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex3')"><i class="fa fa-book"></i> bib</a>
                    <!-- <a class="badge badge-slides" href="slides_link" target="_blank"><i class="fa fa-file-powerpoint-o"></i> slides</a> -->
                    <a class="badge badge-video" href="https://youtu.be/oEz1tEZffWU" target="_blank"><i class="fa fa-video-camera"></i> video</a>
                    <a class="badge badge-code" href="https://github.com/aikesha/EfficientDet-Lite-for-Defect-Detection" target="_blank"><i class="fa fa-code"></i> code</a>
                  </div>
                  <div id="abstract3" class="text-block" style="display: none;">
                    Additive manufacturing is a promising manufacturing process with diverse applications, but ensuring the quality and reliability of the manufactured products are key challenges. The digital twin has emerged as a technology solution to address these challenge, allowing real-time monitoring and control of the manufacturing process. This paper proposes a digital twin system framework for additive manufacturing that integrates machine learning models, employing Unity, OctoPrint, and Raspberry Pi for real-time control and monitoring. Particularly, the system utilizes machine learning models for defect detection, achieving an Average Precision (AP) score of 92%, with specific performance metrics of 91% for defected objects and 94% for non-defected objects, demonstrating high efficiency. The Unity client user interface is also developed for control and visualization, facilitating easy additive manufacturing process monitoring. This research article presents a detailed description of the proposed digital twin framework and its workflow for implementation, the machine learning models, and the Unity client user interface. It also demonstrates the effectiveness of the integrated system through case studies and experimental results. The main findings show that the proposed digital twin system met its functional requirements and effectively detects defects and provides real-time control and monitoring of the additive manufacturing process. This paper contributes to the growing field of digital twin technology and additive manufacturing, providing a promising solution for enhancing the quality and reliability in the field of additive manufacturing.
                  </div>
                  <div id="bibtex3" class="text-block" style="display: none;">
                    @ARTICLE{Jyeniskhan_Digital_Twin_2023,
                    author={Jyeniskhan, Nursultan and Keutayeva, Aigerim and Kazbek, Gani and Ali, Md Hazrat and Shehab, Essam},
                    journal={IEEE Access}, 
                    title={Integrating Machine Learning Model and Digital Twin System for Additive Manufacturing}, 
                    year={2023},
                    volume={11},
                    number={},
                    pages={71113-71126},
                    doi={10.1109/ACCESS.2023.3294486}}
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex3')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    This paper proposes a digital twin system framework for additive manufacturing that integrates machine learning models, employing Unity, OctoPrint, and Raspberry Pi for real-time control and monitoring.
                  </p>
                </td>
              </tr>

              <!-- Publication 6 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src="images/thesis.PNG" alt="clean-usnob" width="160" height="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://nur.nu.edu.kz/handle/123456789/7135">     <!-- LINK:   https://drive.google.com/file/d/1vv04yaW7KutsCoW9ZQ__uNtAiO-fI3n1/view?usp=sharing  -->
                    <span class="papertitle">Robust subject-independent BCIs using Attention Mechanism based Deep Learning models</span>
                  </a>
                  <br>
                  <strong>Aigerim Keutayeva</strong>
                  <br>
                  <em>School of Engineering and Digital Sciences</em>, May 2023
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract4')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex4')"><i class="fa fa-book"></i> bib</a>
                    <a class="badge badge-slides" href="https://drive.google.com/file/d/1urNHf1vjovnrXgLu8vaiJwA_D78977iU/view?usp=sharing" target="_blank"><i class="fa fa-file-powerpoint-o"></i> slides</a>
                  </div>
                  <div id="abstract4" class="text-block" style="display: none;">
                    Brain-Computer Interfaces can revolutionize human-computer interaction by enabling users to engage with technology through cognitive processes. BCIs exhibit extensive prospective applications, comprising the reinstatement of mobility and communication skills in disabled individuals, augmentation of human performance across diverse domains, and provision of novel instruments for scientific exploration. However, one of the significant challenges in developing BCIs is ensuring that they work with different people, regardless of their differences in cognitive abilities, language backgrounds, ages, and physical conditions.
                    This thesis investigates robust subject-independent BCIs using attention mechanismbased deep learning models. The ability to create subject-independent BCIs is crucial for their practical use, as it can reduce the time and cost associated with individual calibration for each user. Additionally, robust subject-independent BCIs can help to improve accessibility for people with severe illnesses, such as amyotrophic lateral sclerosis (ALS), locked-in syndrome, and other conditions that limit mobility and communication abilities.
                    This study uses attention mechanism-based deep learning models to identify the most informative features that are common across all subjects while filtering out noise and irrelevant information. We use two different types of BCI datasets, one based on Event-Related Potentials (ERPs) and the other based on Motor Imagery (MI), to evaluate the performance of our chosen approach. The results show that the attention mechanism-based deep learning models can achieve high levels of accuracy and robustness across different subjects and have the potential to improve the usability of BCIs in various applications.
                  </div>
                  <div id="bibtex4" class="text-block" style="display: none;">
                    @ARTICLE{Keutayeva_thesis_2023,
                    author={Keutayeva, Aigerim},
                    journal={School of Engineering and Digital Sciences}, 
                    title={Robust subject-independent BCIs using Attention Mechanism based Deep Learning models}, 
                    year={2023},
                    volume={},
                    number={},
                    pages={},
                    url={http://nur.nu.edu.kz/handle/123456789/7135}}
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex4')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    This thesis investigates robust subject-independent BCIs using attention mechanismbased deep learning models on 4 ERP- and 4 MI-based BCI datasets. Access by request only.
                  </p>
                </td>
              </tr>
            </tbody></table>


            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h3>Conference Presentations</h3>
              </td>
              </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- Presentation 1 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src='images/ihci2.PNG' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-53827-8_23">
                    <span class="papertitle">Subject-Independent Brain-Computer Interfaces: A Comparative Study of Attention Mechanism-Driven Deep Learning Models</span>
                  </a>
                  <br>
                  <strong>Aigerim Keutayeva</strong>,
                  <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev"> Berdakh Abibullaev</a>
                  <br>
                  <em>15th International Conference on IHCI-2023, EXCO Daegu, Korea</em>, November 8 - 10, 2023
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract5')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex5')"><i class="fa fa-book"></i> bib</a>
                    <a class="badge badge-slides" href="https://drive.google.com/file/d/19Ai-sQOz3rUihUQpHa-cceOpU7yaQzZg/view?usp=sharing" target="_blank"><i class="fa fa-file-powerpoint-o"></i> slides</a>
                    <a class="badge badge-code" href="https://github.com/aikesha/Hybrid-CNN-ViT-EEG" target="_blank"><i class="fa fa-code"></i> code</a>
                  </div>
                  <div id="abstract5" class="text-block" style="display: none;">
                    This research examines the employment of attention mechanism driven deep learning models for building subject-independent Brain-Computer Interfaces (BCIs). The research evaluated three different attention models using the Leave-One-Subject-Out cross-validation method. The results showed that the Hybrid Temporal CNN and ViT model performed well on the BCI competition IV 2a dataset, achieving the highest average accuracy and outperforming other models for 5 out of 9 subjects. However, this model did not perform the best on the BCI competition IV 2b dataset when compared to other methods. One of the challenges faced was the limited size of the data, especially for transformer models that require large amounts of data, which affected the performance variability between datasets. This study highlights a beneficial approach to designing BCIs, combining attention mechanisms with deep learning to extract important inter-subject features from EEG data while filtering out irrelevant signals.
                  </div>
                  <div id="bibtex5" class="text-block" style="display: none;">
                    @inproceedings{keutayeva2023subject,
                    title={Subject-Independent Brain-Computer Interfaces: A Comparative Study of Attention Mechanism-Driven Deep Learning Models},
                    author={Keutayeva, Aigerim and Abibullaev, Berdakh},
                    booktitle={International Conference on Intelligent Human Computer Interaction},
                    pages={245--254},
                    year={2023},
                    organization={Springer}
                    }
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex5')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    This research examines the employment of attention mechanism driven deep learning models for building subject-independent Brain-Computer Interfaces (BCIs).
                  </p>
                </td>
              </tr>
            </tbody></table>



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h3>Book Chapters</h3>
              </td>
              </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- Book Chapter 1 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:top">
                  <img src='images/book-chapter-image.PNG' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">Evolving Trends and Future Prospects of Transformer Models in EEG-based Motor-Imagery BCI Systems</span>
                  </a>
                  <br>
                  <strong>Aigerim Keutayeva</strong>,
                  <a href="https://research.nu.edu.kz/en/persons/amin-zollanvari"> Amin Zollanvari</a>,
                  <a href="https://research.nu.edu.kz/en/persons/berdakh-abibullaev"> Berdakh Abibullaev</a>
                  <br>
                  <em>Discovering the Frontiers of Human-Robot Interaction, Springer</em>, July 24, 2024
                  <br>
                  
                  <br>
                  <div>
                    <a class="badge badge-abs" onclick="showBibtex('abstract6')"><i class="fa fa-file-text-o"></i> abs</a>
                    <a class="badge badge-bib" onclick="showBibtex('bibtex6')"><i class="fa fa-book"></i> bib</a>
                  </div>
                  <div id="abstract6" class="text-block" style="display: none;">
                    This chapter explores the transformative impact of transformer models on EEG-based motor imagery brain-computer interfaces (BCIs)---systems that are pushing the boundaries of human-machine interaction. Transformers, renowned for their self-attention mechanisms, excel at handling sequential data, making them uniquely suited for decoding intricate EEG patterns. We offer a comprehensive review of transformer applications in BCIs, showcasing how they significantly improve signal interpretation accuracy, efficiency, and robustness. The chapter examines the technical foundations, including the inherent complexities of EEG signals---noise, non-stationarity, and intersubject variability---and how transformers tackle them through superior feature extraction and denoising capabilities. We trace the evolution of these models from traditional machine-learning approaches to sophisticated architectures that capture both temporal and spatial dependencies in EEG data. The chapter then delves into practical applications of these models in real-world BCI systems, discussing how they translate into tangible benefits for users. We explore prospects and ongoing research aimed at overcoming limitations like computational demands and the need for personalized models. By analyzing emerging trends and envisioning future directions, this chapter provides a roadmap for the BCI research community, ultimately leading to more intuitive, versatile, and effective human-computer interactions.
                  </div>
                  <div id="bibtex6" class="text-block" style="display: none;">
                    @Inbook{Keutayeva2024,
                    author="Keutayeva, Aigerim and Zollanvari, Amin and Abibullaev, Berdakh",
                    editor="Vinjamuri, Ramana",
                    title="Evolving Trends and Future Prospects of Transformer Models in EEG-Based Motor-Imagery BCI Systems",
                    bookTitle="Discovering the Frontiers of Human-Robot Interaction: Insights and Innovations in Collaboration, Communication, and Control",
                    year="2024",
                    publisher="Springer Nature Switzerland",
                    address="Cham",
                    pages="233--256",
                    isbn="978-3-031-66656-8",
                    doi="10.1007/978-3-031-66656-8_10",
                    url="https://doi.org/10.1007/978-3-031-66656-8_10"}
                    <button class="badge badge-bib" onclick="copyToClipboard('bibtex6')" style="margin-top: 5px;">Copy BibTeX</button>
                  </div>
                  <p>
                    This chapter explores the evolving trends and future potential of Transformer models within EEG-based MI BCIs.
                  </p>
                </td>
              </tr>
            </tbody></table>
          </section>

          <section id="projects">
            <table width="100%" alignment="center" borderline="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>üöÄ Academic Projects</h2>
                  <br>
                  <div class="project-grid">
                    <!-- Project 1 -->
                    <div class="project-card">
                      <img src="images/als_evoked.png" alt="Project 1">
                      <h3><a href="https://drive.google.com/file/d/1sxpjNI6EX4OYs-oR_H65wi3AFpHwYsHq/view?usp=sharing" style="font-weight: bold;">ROBT 613: Brain-Machine Interfaces</a></h3>
                      <p>Designed and implemented an Event-Related Potential-based Brain-Computer Interface classifier
                        using an ensemble model with Linear Discriminant Analysis, Support Vector Classifier, and kNearest Neighbor.</p>
                    </div>

                    <!-- Project 2 -->
                    <div class="project-card">
                      <img src="images/Pipeline.PNG" alt="Project 2">
                      <h3><a href="https://drive.google.com/file/d/1r69GJ4QqDAcnxWiLr8H8DdR2b7KDqabq/view?usp=sharing" style="font-weight: bold;">CSCI 594: Deep Learning</a></h3>
                      <p>Semi-Supervised Multispectral Scene Classification model with Few Labels using
                        MsMatch, EfficientNet Pytorch, and data augmentations, such as Imagio and Albumentations.</p>
                    </div>

                    <!-- Project 3 -->
                    <div class="project-card">
                      <img src="images/epileptic.png" alt="Project 3">
                      <h3><a href="https://github.com/aikesha/Epilectic_Seizure_Dataset" style="font-weight: bold;">ROBT 502: Robot Perception & Vision</a></h3>
                      <p>Convolutional Neural Network-Long Short-Term Memory model (CNN-LSTM) for
                        epileptic seizure recognition using EEG signal analysis.</p>
                    </div>

                    <!-- Project 4 -->
                    <div class="project-card">
                      <img src="images/netflix2.png" alt="Project 4">
                      <h3><a href="path/to/project2-details.html" style="font-weight: bold;">ROBT 407: Machine Learning</a></h3>
                      <p>Support Vector Machines to improve Netflix's recommendation algorithm.</p>
                    </div>

                    <!-- Project 5 -->
                    <div class="project-card">
                      <img src="images/validation_full2.png" alt="Project 5">
                      <h3><a href="https://github.com/aikesha/Child-Action-Recognition-based-on-a-dataset-by-Dr.Sandygulova" style="font-weight: bold;">ROBT 414: Human-Robot Interaction</a></h3>
                      <p>Real-time child-centered action recognition using 2D Skeleton joints with 24 OpenPose body key points with Deep Neural Networks, Recurrent Neural Networks, and Long Short-Term Memory.</p>
                    </div>

                    <!-- Project 6 -->
                    <div class="project-card">
                      <img src="images/robotics_cm_project.png" alt="Project 6">
                      <h3><a href="https://github.com/aikesha/Robotics-II-Control-and-Modelling-Laboratory/tree/master/Laboratory-8" style="font-weight: bold;">ROBT 403: Robotics II. Control, Modeling and Learning</a></h3>
                      <p>Deep Reinforcement Learning by ROS-Gazebo-RViz* to solve IK (Inverse Kinematics) problem.</p>
                    </div>

                    <!-- Project 7 -->
                    <div class="project-card">
                      <img src="images/inverse combo.jpg" alt="Project 7">
                      <h3><a href="https://github.com/aikesha/ROBT-501---Robot-Manipulation-and-Mobility" style="font-weight: bold;">ROBT 501: Robot Manipulation and Mobility</a></h3>
                      <p>Direct and Inverse Kinematics for the Niryo-One Manipulator using Screw Theory.</p>
                    </div>

                    <!-- Project 8 -->
                    <div class="project-card">
                      <img src="images/hw3part1.gif" alt="Project 8">
                      <h3><a href="https://github.com/aikesha/Robotics-II-Obstacle-avoidance-by-using-ANN/tree/main" style="font-weight: bold;">ROBT 403: Robotics II. Control, Modeling and Learning</a></h3>
                      <p>Obstacle avoidance by a robot using ANN.</p>
                    </div>

                  </div>
                </td>
              </tr>
            </tbody></table>
          </section>

          <section id="contact" class="contact-section">
            <table width="100%" alignment="center" borderline="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>ü§ô Contact Me</h2>
                  <br>
                  <div class="contact-container">
                    <div class="contact-card">
                        <a href="mailto:akeutayeva@gmail.comm">
                            <img src="images/email_logo.png" alt="Email">
                            Email
                        </a>
                    </div>
                    <div class="contact-card">
                        <a href="https://github.com/aikesha/">
                            <img src="images/github_logo.png" alt="GitHub">
                            GitHub
                        </a>
                    </div>
                    <div class="contact-card">
                        <a href="https://www.linkedin.com/in/aigerim-keutayeva/">
                            <img src="images/linkedIn_logo.png">
                            LinkedIn
                        </a>
                    </div>
                  </div>

                </td>
              </tr>
            </tbody></table>
          </section>
                
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design was inspired by <a href="https://github.com/jonbarron/website">Jon Barron's</a> and <a href="https://spico197.github.io/">Tong Zhu's</a> websites.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
    <!--<script src="script.js"></script>-->
  </body>
</html>
